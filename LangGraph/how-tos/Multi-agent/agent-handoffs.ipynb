{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # HuggingFace嵌入\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "                    temperature=0,\n",
    "                    max_tokens=4096,\n",
    "                    presence_penalty=1.2,\n",
    "                    # model='deepseek-chat',\n",
    "                    # openai_api_key=os.getenv('DEEPSEEK_API_KEY'),\n",
    "                    # openai_api_base=\"https://api.deepseek.com/v1\",\n",
    "                    model='gpt-4o',\n",
    "                    openai_api_key=os.getenv('CLOSEAI_API_KEY'),\n",
    "                    openai_api_base=\"https://api.openai-proxy.org/v1\",\n",
    "                )\n",
    "\n",
    "# model = ChatOllama(\n",
    "#                     temperature=0,\n",
    "#                     num_predict=4096,\n",
    "#                     repeat_penalty=1.2,\n",
    "#                     model=\"llama3.2:3b\"\n",
    "#                 )\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "            model_name=\"../../../../Embedding/maidalun/bce-embedding-base_v1\",\n",
    "            model_kwargs={\"device\": \"cuda\",\n",
    "                          \"trust_remote_code\": True},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement handoffs using Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Ask multiplication agent for help.\"\"\"\n",
    "    # This tool is not returning anything: we're just using it\n",
    "    # as a way for LLM to signal that it needs to hand off to another agent\n",
    "    # (See the paragraph above)\n",
    "    return\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Ask addition agent for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def addition_expert(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"multiplication_expert\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are an addition expert, you can ask the multiplication expert for help with multiplication. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_multiplication_expert]).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            goto=\"multiplication_expert\", update={\"messages\": [ai_msg, tool_msg]}\n",
    "        )\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "def multiplication_expert(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"addition_expert\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a multiplication expert, you can ask an addition expert for help with addition. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_addition_expert]).invoke(messages)\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"addition_expert\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "# we'll always start with the addition expert\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_messages(update):\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        print(f\"Update from node {node_name}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for m in convert_to_messages(node_update[\"messages\"]):\n",
    "            m.pretty_print()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]},\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement handoffs using tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a handoff tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "def make_handoff_tool(*, agent_name: str):\n",
    "    \"\"\"Create a tool that can return handoff via a Command\"\"\"\n",
    "    tool_name = f\"transfer_to_{agent_name}\"\n",
    "\n",
    "    @tool(tool_name)\n",
    "    def handoff_to_agent(\n",
    "        # # optionally pass current graph state to the tool (will be ignored by the LLM)\n",
    "        state: Annotated[dict, InjectedState],\n",
    "        # optionally pass the current tool call ID (will be ignored by the LLM)\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ):\n",
    "        \"\"\"Ask another agent for help.\"\"\"\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {agent_name}\",\n",
    "            \"name\": tool_name,\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            # navigate to another agent node in the PARENT graph\n",
    "            goto=agent_name,\n",
    "            graph=Command.PARENT,\n",
    "            # This is the state update that the agent `agent_name` will see when it is invoked.\n",
    "            # We're passing agent's FULL internal message history AND adding a tool message to make sure\n",
    "            # the resulting chat history is valid. See the paragraph above for more information.\n",
    "            update={\"messages\": state[\"messages\"] + [tool_message]},\n",
    "        )\n",
    "\n",
    "    return handoff_to_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with a custom agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "def make_agent(model, tools, system_prompt=None):\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "    tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def call_model(state: MessagesState) -> Command[Literal[\"call_tools\", \"__end__\"]]:\n",
    "        messages = state[\"messages\"]\n",
    "        if system_prompt:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "\n",
    "        response = model_with_tools.invoke(messages)\n",
    "        if len(response.tool_calls) > 0:\n",
    "            return Command(goto=\"call_tools\", update={\"messages\": [response]})\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # NOTE: this is a simplified version of the prebuilt ToolNode\n",
    "    # If you want to have a tool node that has full feature parity, please refer to the source code\n",
    "    def call_tools(state: MessagesState) -> Command[Literal[\"call_model\"]]:\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for tool_call in tool_calls:\n",
    "            tool_ = tools_by_name[tool_call[\"name\"]]\n",
    "            tool_input_fields = tool_.get_input_schema().model_json_schema()[\n",
    "                \"properties\"\n",
    "            ]\n",
    "\n",
    "            # this is simplified for demonstration purposes and\n",
    "            # is different from the ToolNode implementation\n",
    "            if \"state\" in tool_input_fields:\n",
    "                # inject state\n",
    "                tool_call = {**tool_call, \"args\": {**tool_call[\"args\"], \"state\": state}}\n",
    "\n",
    "            tool_response = tool_.invoke(tool_call)\n",
    "            if isinstance(tool_response, ToolMessage):\n",
    "                results.append(Command(update={\"messages\": [tool_response]}))\n",
    "\n",
    "            # handle tools that return Command directly\n",
    "            elif isinstance(tool_response, Command):\n",
    "                results.append(tool_response)\n",
    "\n",
    "        # NOTE: nodes in LangGraph allow you to return list of updates, including Command objects\n",
    "        return results\n",
    "\n",
    "    graph = StateGraph(MessagesState)\n",
    "    graph.add_node(call_model)\n",
    "    graph.add_node(call_tools)\n",
    "    graph.add_edge(START, \"call_model\")\n",
    "    graph.add_edge(\"call_tools\", \"call_model\")\n",
    "\n",
    "    return graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = make_agent(model, [add, multiply])\n",
    "\n",
    "for chunk in agent.stream({\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition_expert = make_agent(\n",
    "    model,\n",
    "    [add, make_handoff_tool(agent_name=\"multiplication_expert\")],\n",
    "    system_prompt=\"You are an addition expert, you can ask the multiplication expert for help with multiplication.\",\n",
    ")\n",
    "multiplication_expert = make_agent(\n",
    "    model,\n",
    "    [multiply, make_handoff_tool(agent_name=\"addition_expert\")],\n",
    "    system_prompt=\"You are a multiplication expert, you can ask an addition expert for help with addition.\",\n",
    ")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}, subgraphs=True\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using with a prebuilt ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "addition_expert = create_react_agent(\n",
    "    model,\n",
    "    [add, make_handoff_tool(agent_name=\"multiplication_expert\")],\n",
    "    state_modifier=\"You are an addition expert, you can ask the multiplication expert for help with multiplication.\",\n",
    ")\n",
    "\n",
    "multiplication_expert = create_react_agent(\n",
    "    model,\n",
    "    [multiply, make_handoff_tool(agent_name=\"addition_expert\")],\n",
    "    state_modifier=\"You are a multiplication expert, you can ask an addition expert for help with addition.\",\n",
    ")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}, subgraphs=True\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lclg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
